"""
CloudGatherï¼ˆäº‘é›†ï¼‰- åª’ä½“æ–‡ä»¶åŒæ­¥å·¥å…·
ä½¿ç”¨ Flask + HTML å‰ç«¯
"""

import atexit
import os
import sys
import psutil
import threading
import logging
from logging.handlers import RotatingFileHandler
import requests
import glob
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from pathlib import Path

from flask import Flask, jsonify, render_template, request

from core.scheduler import TaskScheduler
from core.models import SyncTask
from version import __version__

# ç‰ˆæœ¬ä¿¡æ¯
VERSION = __version__

# ç¯å¢ƒé€‚é…ï¼šåˆ¤æ–­æ˜¯å¦åœ¨ Docker ç¯å¢ƒä¸­
IS_DOCKER = os.getenv('IS_DOCKER', 'false').lower() == 'true'

# é…ç½®æ—¥å¿—æ ¼å¼
# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
log_dir = Path('logs')
log_dir.mkdir(exist_ok=True)

# ä»ç¯å¢ƒå˜é‡è¯»å–æ—¥å¿—çº§åˆ«é…ç½®
LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO').upper()  # æ–‡ä»¶æ—¥å¿—çº§åˆ«
CONSOLE_LEVEL = os.getenv('CONSOLE_LEVEL', 'WARNING').upper()  # æ§åˆ¶å°æ—¥å¿—çº§åˆ«ï¼ˆé»˜è®¤åªæ˜¾ç¤ºè­¦å‘Šå’Œé”™è¯¯ï¼‰
LOG_SAVE_DAYS = int(os.getenv('LOG_SAVE_DAYS', '7'))  # æ—¥å¿—ä¿ç•™å¤©æ•°

# é…ç½® root logger
root_logger = logging.getLogger()
root_logger.setLevel(logging.DEBUG)  # è®¾ç½®ä¸º DEBUG ä»¥æ•æ‰æ‰€æœ‰çº§åˆ«

# æ–‡ä»¶ handler - ä¿å­˜æ‰€æœ‰æ—¥å¿—
file_handler = RotatingFileHandler(
    log_dir / 'cloudgather.log',
    maxBytes=10*1024*1024,  # 10MB
    backupCount=5,
    encoding='utf-8'
)
file_handler.setLevel(getattr(logging, LOG_LEVEL, logging.INFO))
file_formatter = logging.Formatter(
    '[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
file_handler.setFormatter(file_formatter)
root_logger.addHandler(file_handler)

# æ§åˆ¶å° handler - é»˜è®¤åªæ˜¾ç¤ºè­¦å‘Šå’Œé”™è¯¯ï¼Œä¸æ˜¾ç¤ºä»»åŠ¡æ‰§è¡Œä¿¡æ¯
console_handler = logging.StreamHandler(sys.stdout)  # æ˜¾å¼æŒ‡å®š stdout
console_handler.setLevel(getattr(logging, CONSOLE_LEVEL, logging.WARNING))
console_formatter = logging.Formatter(
    '[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
console_handler.setFormatter(console_formatter)
root_logger.addHandler(console_handler)

# Docker ç¯å¢ƒä¸‹å¼ºåˆ¶åˆ·æ–°è¾“å‡º
if IS_DOCKER:
    sys.stdout.reconfigure(line_buffering=True)
    sys.stderr.reconfigure(line_buffering=True)


def cleanup_old_logs():
    """æ¸…ç†è¿‡æœŸæ—¥å¿—æ–‡ä»¶"""
    try:
        cutoff_time = time.time() - (LOG_SAVE_DAYS * 86400)  # è½¬æ¢ä¸ºç§’
        log_files = glob.glob(str(log_dir / 'cloudgather.log.*'))
        
        removed_count = 0
        for log_file in log_files:
            try:
                file_path = Path(log_file)
                if file_path.stat().st_mtime < cutoff_time:
                    file_path.unlink()
                    removed_count += 1
            except Exception as e:
                # åªå†™å…¥æ–‡ä»¶ï¼Œä¸è¾“å‡ºæ§åˆ¶å°
                file_handler.handle(logging.LogRecord(
                    name='cloudgather',
                    level=logging.WARNING,
                    pathname='',
                    lineno=0,
                    msg=f"åˆ é™¤è¿‡æœŸæ—¥å¿—å¤±è´¥: {log_file} - {e}",
                    args=(),
                    exc_info=None
                ))
        
        # æ¸…ç†æˆåŠŸä¿¡æ¯åªå†™å…¥æ–‡ä»¶
        if removed_count > 0:
            file_handler.handle(logging.LogRecord(
                name='cloudgather',
                level=logging.INFO,
                pathname='',
                lineno=0,
                msg=f"âœ… å·²æ¸…ç† {removed_count} ä¸ªè¿‡æœŸæ—¥å¿—æ–‡ä»¶",
                args=(),
                exc_info=None
            ))
    except Exception as e:
        # é”™è¯¯ä¿¡æ¯åªå†™å…¥æ–‡ä»¶
        file_handler.handle(logging.LogRecord(
            name='cloudgather',
            level=logging.ERROR,
            pathname='',
            lineno=0,
            msg=f"æ¸…ç†æ—¥å¿—å¤±è´¥: {e}",
            args=(),
            exc_info=None
        ))


# å¯åŠ¨æ—¶æ¸…ç†ä¸€æ¬¡è¿‡æœŸæ—¥å¿—
cleanup_old_logs()

# ç¯å¢ƒé€‚é…ï¼šé…ç½®è·¯å¾„
CONFIG_PATH = '/app/config/tasks.json' if IS_DOCKER else 'config/tasks.json'

# å…¨å±€è°ƒåº¦å™¨å®ä¾‹
scheduler = TaskScheduler(config_path=CONFIG_PATH)

# æ—¥å¿—å­˜å‚¨
log_lock = threading.Lock()
MAX_LOGS = 500
_task_logs: Dict[str, List[str]] = {"general": []}  # task_id -> logs
_current_task_id: Optional[str] = None  # å½“å‰æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡ID


def log_handler(message: str):
    """ç»Ÿä¸€æ—¥å¿—å¤„ç†å™¨ï¼Œå­˜å…¥å†…å­˜ä¾›å‰ç«¯æ‹‰å–ï¼Œåªå†™æ–‡ä»¶ä¸è¾“å‡ºæ§åˆ¶å°"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    entry = f"[{timestamp}] {message}"
    
    # åªå†™å…¥æ–‡ä»¶æ—¥å¿—ï¼Œä¸è¾“å‡ºåˆ°æ§åˆ¶å°
    file_handler.handle(logging.LogRecord(
        name='cloudgather',
        level=logging.INFO,
        pathname='',
        lineno=0,
        msg=message,
        args=(),
        exc_info=None
    ))
    
    with log_lock:
        # æ·»åŠ åˆ°å…¨å±€æ—¥å¿—
        logs = _task_logs.setdefault('general', [])
        logs.append(entry)
        if len(logs) > MAX_LOGS:
            _task_logs['general'] = logs[-MAX_LOGS:]
        
        # å¦‚æœæœ‰å½“å‰ä»»åŠ¡ï¼Œä¹Ÿæ·»åŠ åˆ°ä»»åŠ¡ä¸“å±æ—¥å¿—
        if _current_task_id:
            task_logs = _task_logs.setdefault(_current_task_id, [])
            task_logs.append(entry)
            if len(task_logs) > MAX_LOGS:
                _task_logs[_current_task_id] = task_logs[-MAX_LOGS:]


def set_current_task(task_id: Optional[str]):
    """è®¾ç½®å½“å‰æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡ID"""
    global _current_task_id
    _current_task_id = task_id


# ç»‘å®šè°ƒåº¦å™¨æ—¥å¿—
scheduler.set_log_callback(log_handler)
scheduler.set_task_context_callback(set_current_task)

# å¯åŠ¨è°ƒåº¦å™¨ï¼ˆå¹‚ç­‰ï¼‰
def ensure_scheduler_running():
    if not scheduler.is_running:
        scheduler.start()


ensure_scheduler_running()

# Flask åº”ç”¨
app = Flask(__name__, static_folder='static', template_folder='html')

# è®¾ç½® Werkzeug æ—¥å¿—ï¼šå°† HTTP è®¿é—®æ—¥å¿—å‹ä½åˆ° WARNING çº§åˆ«
werkzeug_logger = logging.getLogger('werkzeug')
werkzeug_logger.setLevel(logging.WARNING)  # åªæ˜¾ç¤ºè­¦å‘Šå’Œé”™è¯¯
werkzeug_logger.propagate = True  # ä¼ é€’åˆ° root loggerï¼Œç¡®ä¿è­¦å‘Š/é”™è¯¯ä¼šè¢«è®°å½•åˆ°æ–‡ä»¶

# æ³¨å†Œ API è“å›¾
from api.status import status_bp, init_status_bp
from api.tasks import tasks_bp, init_tasks_bp
from api.settings import settings_bp, init_settings_bp
from api.strm import strm_bp, init_strm_bp

init_status_bp(scheduler, CONFIG_PATH, IS_DOCKER, VERSION)
init_tasks_bp(scheduler, log_handler, IS_DOCKER, _task_logs, log_lock)
init_settings_bp(IS_DOCKER)
init_strm_bp(scheduler, log_handler)

app.register_blueprint(status_bp, url_prefix='/api')
app.register_blueprint(tasks_bp, url_prefix='/api')
app.register_blueprint(settings_bp, url_prefix='/api')
app.register_blueprint(strm_bp, url_prefix='/api')


@app.route('/')
def index():
    return render_template('index.html')


@atexit.register
def _cleanup():
    """é€€å‡ºæ—¶æ¸…ç†èµ„æº"""
    if scheduler.is_running:
        scheduler.stop()


def fetch_hitokoto():
    """è·å–ä¸€è¨€"""
    try:
        response = requests.get('https://v1.hitokoto.cn/', timeout=5)
        data = response.json()
        text = data.get('hitokoto', 'ä»Šå¤©ä¹Ÿè¦åŠ æ²¹å“¦ï¼')
        from_who = data.get('from', '')
        return f"{text} â€”â€” {from_who}" if from_who else text
    except Exception as e:
        return 'ä¿æŒçƒ­çˆ±ï¼Œå¥”èµ´å±±æµ·'


if __name__ == '__main__':
    # è·å–ä¸€è¨€
    hitokoto = fetch_hitokoto()
    
    # åªåœ¨é debug æ¨¡å¼æˆ–ä¸»è¿›ç¨‹ä¸­æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
    # debug æ¨¡å¼ä¸‹ï¼Œos.environ.get('WERKZEUG_RUN_MAIN') åªåœ¨å­è¿›ç¨‹ä¸­ä¸º 'true'
    if IS_DOCKER or os.environ.get('WERKZEUG_RUN_MAIN') == 'true':
        # å¯åŠ¨ä¿¡æ¯
        print(f'\nâœ… CloudGather v{VERSION} å¯åŠ¨æˆåŠŸ')
        print(f'â° æ—¶åŒº: {os.getenv("TZ", "UTC")}')
        print(f'ğŸŒ è®¿é—®åœ°å€: http://127.0.0.1:3602')
        print(f'ğŸ’¬ ä¸€è¨€: {hitokoto}')
        print('â–¶ï¸  æœåŠ¡è¿è¡Œä¸­... (æŒ‰ CTRL+C åœæ­¢)\n')
    
    # å¯åŠ¨ Flask
    app.run(
        host='0.0.0.0' if IS_DOCKER else '127.0.0.1',
        port=3602,
        debug=not IS_DOCKER
    )
