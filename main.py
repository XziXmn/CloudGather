"""
CloudGatherï¼ˆäº‘é›†ï¼‰- åª’ä½“æ–‡ä»¶åŒæ­¥å·¥å…·
ä½¿ç”¨ Flask + HTML å‰ç«¯
Version: 0.2
"""

import atexit
import os
import psutil
import threading
import logging
from logging.handlers import RotatingFileHandler
import requests
import glob
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from pathlib import Path

from flask import Flask, jsonify, render_template, request

from core.scheduler import TaskScheduler
from core.models import SyncTask
from version import __version__

# ç‰ˆæœ¬ä¿¡æ¯
VERSION = __version__

# é…ç½®æ—¥å¿—æ ¼å¼
# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
log_dir = Path('logs')
log_dir.mkdir(exist_ok=True)

# ä»ç¯å¢ƒå˜é‡è¯»å–æ—¥å¿—çº§åˆ«é…ç½®
LOG_LEVEL = os.getenv('LOG_LEVEL', 'INFO').upper()  # æ–‡ä»¶æ—¥å¿—çº§åˆ«
CONSOLE_LEVEL = os.getenv('CONSOLE_LEVEL', 'INFO').upper()  # æ§åˆ¶å°æ—¥å¿—çº§åˆ«
LOG_SAVE_DAYS = int(os.getenv('LOG_SAVE_DAYS', '7'))  # æ—¥å¿—ä¿ç•™å¤©æ•°

# é…ç½® root logger
root_logger = logging.getLogger()
root_logger.setLevel(logging.DEBUG)  # è®¾ç½®ä¸º DEBUG ä»¥æ•æ‰æ‰€æœ‰çº§åˆ«

# æ–‡ä»¶ handler - ä¿å­˜æ‰€æœ‰æ—¥å¿—
file_handler = RotatingFileHandler(
    log_dir / 'cloudgather.log',
    maxBytes=10*1024*1024,  # 10MB
    backupCount=5,
    encoding='utf-8'
)
file_handler.setLevel(getattr(logging, LOG_LEVEL, logging.INFO))
file_formatter = logging.Formatter(
    '[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
file_handler.setFormatter(file_formatter)
root_logger.addHandler(file_handler)

# æ§åˆ¶å° handler - åªæ˜¾ç¤ºä¸šåŠ¡æ—¥å¿—
console_handler = logging.StreamHandler()
console_handler.setLevel(getattr(logging, CONSOLE_LEVEL, logging.INFO))
console_formatter = logging.Formatter(
    '[%(asctime)s] %(levelname)s: %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
console_handler.setFormatter(console_formatter)
root_logger.addHandler(console_handler)


def cleanup_old_logs():
    """æ¸…ç†è¿‡æœŸæ—¥å¿—æ–‡ä»¶"""
    try:
        cutoff_time = time.time() - (LOG_SAVE_DAYS * 86400)  # è½¬æ¢ä¸ºç§’
        log_files = glob.glob(str(log_dir / 'cloudgather.log.*'))
        
        removed_count = 0
        for log_file in log_files:
            try:
                file_path = Path(log_file)
                if file_path.stat().st_mtime < cutoff_time:
                    file_path.unlink()
                    removed_count += 1
            except Exception as e:
                logging.warning(f"åˆ é™¤è¿‡æœŸæ—¥å¿—å¤±è´¥: {log_file} - {e}")
        
        if removed_count > 0:
            logging.info(f"âœ… å·²æ¸…ç† {removed_count} ä¸ªè¿‡æœŸæ—¥å¿—æ–‡ä»¶")
    except Exception as e:
        logging.error(f"æ¸…ç†æ—¥å¿—å¤±è´¥: {e}")


# å¯åŠ¨æ—¶æ¸…ç†ä¸€æ¬¡è¿‡æœŸæ—¥å¿—
cleanup_old_logs()

# ç¯å¢ƒé€‚é…ï¼šåˆ¤æ–­æ˜¯å¦åœ¨ Docker ç¯å¢ƒä¸­
IS_DOCKER = os.getenv('IS_DOCKER', 'false').lower() == 'true'
CONFIG_PATH = '/app/config/tasks.json' if IS_DOCKER else 'config/tasks.json'

# å…¨å±€è°ƒåº¦å™¨å®ä¾‹
scheduler = TaskScheduler(config_path=CONFIG_PATH)

# æ—¥å¿—å­˜å‚¨
log_lock = threading.Lock()
MAX_LOGS = 500
_task_logs: Dict[str, List[str]] = {"general": []}  # task_id -> logs
_current_task_id: Optional[str] = None  # å½“å‰æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡ID


def log_handler(message: str):
    """ç»Ÿä¸€æ—¥å¿—å¤„ç†å™¨ï¼Œå­˜å…¥å†…å­˜ä¾›å‰ç«¯æ‹‰å–"""
    timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    entry = f"[{timestamp}] {message}"
    with log_lock:
        # æ·»åŠ åˆ°å…¨å±€æ—¥å¿—
        logs = _task_logs.setdefault('general', [])
        logs.append(entry)
        if len(logs) > MAX_LOGS:
            _task_logs['general'] = logs[-MAX_LOGS:]
        
        # å¦‚æœæœ‰å½“å‰ä»»åŠ¡ï¼Œä¹Ÿæ·»åŠ åˆ°ä»»åŠ¡ä¸“å±æ—¥å¿—
        if _current_task_id:
            task_logs = _task_logs.setdefault(_current_task_id, [])
            task_logs.append(entry)
            if len(task_logs) > MAX_LOGS:
                _task_logs[_current_task_id] = task_logs[-MAX_LOGS:]


def set_current_task(task_id: Optional[str]):
    """è®¾ç½®å½“å‰æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡ID"""
    global _current_task_id
    _current_task_id = task_id


# ç»‘å®šè°ƒåº¦å™¨æ—¥å¿—
scheduler.set_log_callback(log_handler)
scheduler.set_task_context_callback(set_current_task)

# å¯åŠ¨è°ƒåº¦å™¨ï¼ˆå¹‚ç­‰ï¼‰
def ensure_scheduler_running():
    if not scheduler.is_running:
        scheduler.start()


ensure_scheduler_running()

# Flask åº”ç”¨
app = Flask(__name__, static_folder='static', template_folder='html')

# è®¾ç½® Werkzeug æ—¥å¿—ï¼šå°† HTTP è®¿é—®æ—¥å¿—å‹ä½åˆ° WARNING çº§åˆ«
werkzeug_logger = logging.getLogger('werkzeug')
werkzeug_logger.setLevel(logging.WARNING)  # åªæ˜¾ç¤ºè­¦å‘Šå’Œé”™è¯¯
werkzeug_logger.propagate = True  # ä¼ é€’åˆ° root loggerï¼Œç¡®ä¿è­¦å‘Š/é”™è¯¯ä¼šè¢«è®°å½•åˆ°æ–‡ä»¶


@app.route('/')
def index():
    return render_template('index.html')


def _task_to_dict(task: SyncTask) -> dict:
    data = task.to_dict()
    # æ·»åŠ ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´
    next_run_time = scheduler.get_next_run_time(task.id)
    if next_run_time:
        data['next_run_time'] = next_run_time.isoformat()
    else:
        data['next_run_time'] = None
    
    # æ·»åŠ ä»»åŠ¡è¿›åº¦ï¼ˆå¦‚æœæ­£åœ¨æ‰§è¡Œï¼‰
    if task.status.value == 'RUNNING' and task.id in scheduler.task_progress:
        data['progress'] = scheduler.task_progress[task.id]
    
    # æ·»åŠ æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    if task.id in scheduler.task_stats:
        data['stats'] = scheduler.task_stats[task.id]
    
    return data


def _parse_bool(value, default=False):
    if isinstance(value, bool):
        return value
    if isinstance(value, str):
        return value.lower() in {'true', '1', 'yes', 'on'}
    return default


def _validate_paths_for_request(source_path: str, target_path: str):
    """æ ¡éªŒæº/ç›®æ ‡è·¯å¾„å¯ç”¨æ€§ï¼Œå¹¶åœ¨éœ€è¦æ—¶åˆ›å»ºç›®æ ‡ç›®å½•"""
    source = Path(source_path)
    target = Path(target_path)
    
    if not source.exists():
        return False, f"æºç›®å½•ä¸å­˜åœ¨: {source}"
    if not source.is_dir():
        return False, f"æºè·¯å¾„ä¸æ˜¯ç›®å½•: {source}"
    if not os.access(source, os.R_OK):
        return False, f"æ²¡æœ‰è¯»å–æºç›®å½•çš„æƒé™: {source}"
    
    try:
        if not target.exists():
            target.mkdir(parents=True, exist_ok=True)
        if not target.is_dir():
            return False, f"ç›®æ ‡è·¯å¾„ä¸æ˜¯ç›®å½•: {target}"
        if not os.access(target, os.W_OK):
            return False, f"æ²¡æœ‰å†™å…¥ç›®æ ‡ç›®å½•çš„æƒé™: {target}"
    except PermissionError as e:
        return False, f"æ— æ³•åˆ›å»º/è®¿é—®ç›®æ ‡ç›®å½•: {e}"
    except Exception as e:
        return False, f"ç›®æ ‡ç›®å½•æ£€æŸ¥å¤±è´¥: {e}"
    
    return True, None


@app.route('/api/status', methods=['GET'])
def api_status():
    # è·å–ç³»ç»Ÿèµ„æºä¿¡æ¯
    memory = psutil.virtual_memory()
    cpu_percent = psutil.cpu_percent(interval=0.1)
    disk_usage = psutil.disk_usage('/')
    
    # ç»Ÿè®¡ä»»åŠ¡çŠ¶æ€
    task_stats = {
        'total': len(scheduler.tasks),
        'enabled': sum(1 for t in scheduler.tasks.values() if t.enabled),
        'disabled': sum(1 for t in scheduler.tasks.values() if not t.enabled),
        'idle': sum(1 for t in scheduler.tasks.values() if t.status.value == 'IDLE'),
        'running': sum(1 for t in scheduler.tasks.values() if t.status.value == 'RUNNING'),
        'queued': sum(1 for t in scheduler.tasks.values() if t.status.value == 'QUEUED'),
        'error': sum(1 for t in scheduler.tasks.values() if t.status.value == 'ERROR')
    }
    
    # è·å–æœ€è¿‘æ‰§è¡Œä»»åŠ¡
    recent_tasks = sorted(
        [t for t in scheduler.tasks.values() if t.last_run_time],
        key=lambda x: x.last_run_time or '',
        reverse=True
    )[:5]
    
    # é…ç½®æ–‡ä»¶ä¿¡æ¯
    config_stat = None
    if Path(CONFIG_PATH).exists():
        stat = Path(CONFIG_PATH).stat()
        config_stat = {
            'size': stat.st_size,
            'modified': datetime.fromtimestamp(stat.st_mtime).isoformat()
        }
    
    return jsonify({
        'running': scheduler.is_running,
        'queue_size': scheduler.get_queue_size(),
        'task_count': len(scheduler.tasks),
        'config_path': str(CONFIG_PATH),
        'is_docker': IS_DOCKER,
        'version': VERSION,
        
        # é…ç½®å¥åº·çŠ¶æ€
        'config_health': {
            'exists': Path(CONFIG_PATH).exists(),
            'dir_exists': Path(CONFIG_PATH).parent.exists(),
            'dir_writable': os.access(Path(CONFIG_PATH).parent, os.W_OK),
            'file_writable': os.access(Path(CONFIG_PATH), os.W_OK) if Path(CONFIG_PATH).exists() else None,
            'file_stat': config_stat
        },
        
        # ä»»åŠ¡ç»Ÿè®¡
        'task_stats': task_stats,
        
        # æœ€è¿‘æ‰§è¡Œçš„ä»»åŠ¡
        'recent_tasks': [
            {
                'id': t.id,
                'name': t.name,
                'last_run_time': t.last_run_time,
                'status': t.status.value
            } for t in recent_tasks
        ],
        
        # ç³»ç»Ÿèµ„æº
        'system': {
            'cpu_percent': cpu_percent,
            'memory_total': memory.total,
            'memory_used': memory.used,
            'memory_percent': memory.percent,
            'memory_available': memory.available,
            'disk_total': disk_usage.total,
            'disk_used': disk_usage.used,
            'disk_free': disk_usage.free,
            'disk_percent': disk_usage.percent
        }
    })


@app.route('/api/tasks', methods=['GET', 'POST'])
def api_tasks():
    if request.method == 'GET':
        tasks = [_task_to_dict(t) for t in scheduler.get_all_tasks()]
        return jsonify({'tasks': tasks})

    data = request.get_json(silent=True) or {}
    required_fields = ['name', 'source_path', 'target_path']
    missing = [f for f in required_fields if f not in data]
    if missing:
        return jsonify({'success': False, 'error': f"ç¼ºå°‘å­—æ®µ: {', '.join(missing)}"}), 400

    source_path = data['source_path']
    target_path = data['target_path']
    ok, err = _validate_paths_for_request(source_path, target_path)
    if not ok:
        return jsonify({'success': False, 'error': err}), 400

    # è·å–è°ƒåº¦ç±»å‹
    schedule_type = data.get('schedule_type', 'INTERVAL')
    
    if schedule_type == 'CRON':
        # Cron è°ƒåº¦
        cron_expression = data.get('cron_expression', '').strip()
        if not cron_expression:
            return jsonify({'success': False, 'error': 'Cron è¡¨è¾¾å¼ä¸èƒ½ä¸ºç©º'}), 400
        
        # éªŒè¯ cron è¡¨è¾¾å¼æ ¼å¼
        parts = cron_expression.split()
        if len(parts) != 5:
            return jsonify({'success': False, 'error': 'Cron è¡¨è¾¾å¼æ ¼å¼é”™è¯¯ï¼Œåº”ä¸º 5 ä¸ªå­—æ®µï¼šåˆ† æ—¶ æ—¥ æœˆ æ˜ŸæœŸ'}), 400
        
        task = SyncTask(
            name=data['name'],
            source_path=source_path,
            target_path=target_path,
            schedule_type='CRON',
            cron_expression=cron_expression,
            interval=300,  # cron æ¨¡å¼ä¸‹ interval ä¸ä½¿ç”¨ï¼Œä½†éœ€è¦é»˜è®¤å€¼
            enabled=_parse_bool(data.get('enabled', True), True),
            overwrite_existing=_parse_bool(data.get('overwrite_existing', False), False),
            thread_count=int(data.get('thread_count', 1)),
            rule_not_exists=_parse_bool(data.get('rule_not_exists', False), False),
            rule_size_diff=_parse_bool(data.get('rule_size_diff', False), False),
            rule_mtime_newer=_parse_bool(data.get('rule_mtime_newer', False), False),
            is_slow_storage=_parse_bool(data.get('is_slow_storage', False), False)
        )
    else:
        # é—´éš”è°ƒåº¦
        try:
            interval = int(data.get('interval', 300))
        except ValueError:
            return jsonify({'success': False, 'error': 'åŒæ­¥é—´éš”å¿…é¡»æ˜¯æ•°å­—'}), 400

        if interval < 5:
            return jsonify({'success': False, 'error': 'åŒæ­¥é—´éš”éœ€å¤§äºç­‰äº 5 ç§’'}), 400

        task = SyncTask(
            name=data['name'],
            source_path=source_path,
            target_path=target_path,
            schedule_type='INTERVAL',
            interval=interval,
            enabled=_parse_bool(data.get('enabled', True), True),
            overwrite_existing=_parse_bool(data.get('overwrite_existing', False), False),
            thread_count=int(data.get('thread_count', 1)),
            rule_not_exists=_parse_bool(data.get('rule_not_exists', False), False),
            rule_size_diff=_parse_bool(data.get('rule_size_diff', False), False),
            rule_mtime_newer=_parse_bool(data.get('rule_mtime_newer', False), False),
            is_slow_storage=_parse_bool(data.get('is_slow_storage', False), False)
        )

    if scheduler.add_task(task):
        return jsonify({'success': True, 'task': _task_to_dict(task)})
    return jsonify({'success': False, 'error': 'æ·»åŠ ä»»åŠ¡å¤±è´¥'}), 500


@app.route('/api/tasks/<task_id>', methods=['PUT', 'DELETE'])
def api_task_detail(task_id: str):
    task = scheduler.get_task(task_id)
    if not task:
        return jsonify({'success': False, 'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404

    if request.method == 'DELETE':
        if scheduler.remove_task(task_id):
            return jsonify({'success': True})
        return jsonify({'success': False, 'error': 'åˆ é™¤ä»»åŠ¡å¤±è´¥'}), 500

    data = request.get_json(silent=True) or {}
    updates: Dict[str, Optional[object]] = {}

    if 'name' in data:
        updates['name'] = data['name']
    if 'source_path' in data:
        updates['source_path'] = data['source_path']
    if 'target_path' in data:
        updates['target_path'] = data['target_path']
    if 'interval' in data:
        try:
            updates['interval'] = int(data['interval'])
        except ValueError:
            return jsonify({'success': False, 'error': 'åŒæ­¥é—´éš”å¿…é¡»æ˜¯æ•°å­—'}), 400
    if 'enabled' in data:
        updates['enabled'] = _parse_bool(data['enabled'], task.enabled)
    if 'overwrite_existing' in data:
        updates['overwrite_existing'] = _parse_bool(data['overwrite_existing'], task.overwrite_existing)
    if 'thread_count' in data:
        try:
            updates['thread_count'] = max(1, int(data['thread_count']))
        except ValueError:
            return jsonify({'success': False, 'error': 'çº¿ç¨‹æ•°å¿…é¡»æ˜¯æ•°å­—'}), 400
    if 'rule_not_exists' in data:
        updates['rule_not_exists'] = _parse_bool(data['rule_not_exists'], task.rule_not_exists)
    if 'rule_size_diff' in data:
        updates['rule_size_diff'] = _parse_bool(data['rule_size_diff'], task.rule_size_diff)
    if 'rule_mtime_newer' in data:
        updates['rule_mtime_newer'] = _parse_bool(data['rule_mtime_newer'], task.rule_mtime_newer)
    if 'is_slow_storage' in data:
        updates['is_slow_storage'] = _parse_bool(data['is_slow_storage'], task.is_slow_storage)

    # è·¯å¾„æ›´æ–°æ—¶æ ¡éªŒå¹¶åˆ›å»ºç›®æ ‡ç›®å½•
    if 'source_path' in updates or 'target_path' in updates:
        new_source = updates.get('source_path', task.source_path)
        new_target = updates.get('target_path', task.target_path)
        ok, err = _validate_paths_for_request(new_source, new_target)
        if not ok:
            return jsonify({'success': False, 'error': err}), 400

    if 'interval' in updates and updates['interval'] is not None and updates['interval'] < 5:
        return jsonify({'success': False, 'error': 'åŒæ­¥é—´éš”éœ€å¤§äºç­‰äº 5 ç§’'}), 400

    if scheduler.update_task(task_id, **updates):
        updated = scheduler.get_task(task_id)
        return jsonify({'success': True, 'task': _task_to_dict(updated)})
    return jsonify({'success': False, 'error': 'æ›´æ–°ä»»åŠ¡å¤±è´¥'}), 500


@app.route('/api/tasks/<task_id>/trigger', methods=['POST'])
def api_trigger_task(task_id: str):
    if scheduler.trigger_task_now(task_id):
        return jsonify({'success': True})
    return jsonify({'success': False, 'error': 'ä»»åŠ¡çŠ¶æ€éç©ºé—²æˆ–ä¸å­˜åœ¨'}), 400


@app.route('/api/tasks/<task_id>/full-overwrite', methods=['POST'])
def api_full_overwrite_task(task_id: str):
    """å…¨é‡è¦†ç›–ï¼šä¸€æ¬¡æ€§å¼ºåˆ¶è¦†ç›–æ‰€æœ‰å·²å­˜åœ¨æ–‡ä»¶"""
    task = scheduler.get_task(task_id)
    if not task:
        return jsonify({'success': False, 'error': 'ä»»åŠ¡ä¸å­˜åœ¨'}), 404
    
    if task.status != 'IDLE':
        return jsonify({'success': False, 'error': 'ä»»åŠ¡çŠ¶æ€éç©ºé—²ï¼Œæ— æ³•æ‰§è¡Œ'}), 400
    
    # ä½¿ç”¨ç°æœ‰çš„ trigger_task_now æœºåˆ¶ï¼Œä½†ä¼ é€’ç‰¹æ®Šæ ‡è®°
    # æ³¨ï¼šè¿™é‡Œéœ€è¦ä¿®æ”¹ä»»åŠ¡çš„ overwrite_existing ä¸º Trueï¼Œæ‰§è¡Œåæ¢å¤
    original_overwrite = task.overwrite_existing
    task.overwrite_existing = True  # ä¸´æ—¶è®¾ç½®ä¸ºè¦†ç›–æ¨¡å¼
    
    # è®°å½•æ—¥å¿—
    log_handler(f"ğŸ”¥ å¼€å§‹æ‰§è¡Œå…¨é‡è¦†ç›–: {task.name}")
    
    # è§¦å‘ä»»åŠ¡
    success = scheduler.trigger_task_now(task_id)
    
    if success:
        # åœ¨åå°æ¢å¤åŸå§‹è®¾ç½®ï¼ˆä¸ä¿å­˜åˆ°æ–‡ä»¶ï¼‰
        # æ³¨ï¼šä»»åŠ¡æ‰§è¡Œå®Œåä¼šè‡ªåŠ¨æ¢å¤
        import threading
        def reset_overwrite():
            import time
            time.sleep(1)  # ç­‰å¾…ä»»åŠ¡å¼€å§‹æ‰§è¡Œ
            task.overwrite_existing = original_overwrite
        threading.Thread(target=reset_overwrite, daemon=True).start()
        
        return jsonify({'success': True})
    else:
        task.overwrite_existing = original_overwrite  # æ¢å¤
        return jsonify({'success': False, 'error': 'è§¦å‘ä»»åŠ¡å¤±è´¥'}), 500


@app.route('/api/scheduler/start', methods=['POST'])
def api_start_scheduler():
    ensure_scheduler_running()
    return jsonify({'success': True, 'running': scheduler.is_running})


@app.route('/api/scheduler/stop', methods=['POST'])
def api_stop_scheduler():
    if scheduler.is_running:
        scheduler.stop()
    return jsonify({'success': True, 'running': scheduler.is_running})


@app.route('/api/queue', methods=['GET'])
def api_queue():
    """è·å–å½“å‰ä»»åŠ¡é˜Ÿåˆ—ä¿¡æ¯"""
    queue_tasks = []
    # è·å–é˜Ÿåˆ—ä¸­çš„ä»»åŠ¡ï¼ˆä¸ç§»é™¤ï¼‰
    with scheduler.task_queue.mutex:
        queue_list = list(scheduler.task_queue.queue)
    
    for task_id in queue_list:
        task = scheduler.get_task(task_id)
        if task:
            queue_tasks.append(_task_to_dict(task))
    
    return jsonify({'queue': queue_tasks})


@app.route('/api/logs', methods=['GET'])
def api_logs():
    task_id = request.args.get('task_id', 'general')
    with log_lock:
        logs = list(_task_logs.get(task_id, []))
    return jsonify({'logs': logs})


@app.route('/api/logs/clear', methods=['POST'])
def api_clear_logs():
    task_id = request.args.get('task_id', 'general')
    with log_lock:
        if task_id in _task_logs:
            _task_logs[task_id] = []
    return jsonify({'success': True})


@app.route('/api/directories', methods=['GET'])
def api_list_directories():
    """åˆ—å‡ºæŒ‡å®šè·¯å¾„ä¸‹çš„ç›®å½•"""
    path = request.args.get('path', '/')
    try:
        from pathlib import Path
        import os
        
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿è·¯å¾„å­˜åœ¨
        target_path = Path(path)
        if not target_path.exists():
            # å¦‚æœè·¯å¾„ä¸å­˜åœ¨ï¼Œè¿”å›çˆ¶ç›®å½•
            parent = target_path.parent
            if parent.exists() and parent.is_dir():
                target_path = parent
            else:
                # è¿”å›æ ¹ç›®å½•æˆ–ç”¨æˆ·ä¸»ç›®å½•
                target_path = Path('/') if IS_DOCKER else Path.home()
        
        # åªåˆ—å‡ºç›®å½•
        dirs = []
        if target_path.is_dir():
            try:
                for item in sorted(target_path.iterdir()):
                    if item.is_dir():
                        try:
                            # æ£€æŸ¥æ˜¯å¦å¯è¯»
                            item.stat()
                            dirs.append({
                                'name': item.name,
                                'path': str(item),
                                'parent': str(item.parent)
                            })
                        except (PermissionError, OSError):
                            # è·³è¿‡æ— æƒé™çš„ç›®å½•
                            continue
            except PermissionError:
                return jsonify({
                    'success': False,
                    'error': 'æ²¡æœ‰æƒé™è®¿é—®æ­¤ç›®å½•',
                    'current_path': str(target_path),
                    'directories': []
                })
        
        return jsonify({
            'success': True,
            'current_path': str(target_path),
            'parent_path': str(target_path.parent) if target_path.parent != target_path else None,
            'directories': dirs
        })
    except Exception as e:
        return jsonify({
            'success': False,
            'error': str(e),
            'current_path': path,
            'directories': []
        })


@app.route('/api/cron/presets', methods=['GET'])
def api_cron_presets():
    """è·å– Cron è¡¨è¾¾å¼é¢„è®¾"""
    presets = [
        {'name': 'æ¯ 5 åˆ†é’Ÿ', 'expression': '*/5 * * * *', 'description': 'æ¯ 5 åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡'},
        {'name': 'æ¯ 10 åˆ†é’Ÿ', 'expression': '*/10 * * * *', 'description': 'æ¯ 10 åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡'},
        {'name': 'æ¯ 15 åˆ†é’Ÿ', 'expression': '*/15 * * * *', 'description': 'æ¯ 15 åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡'},
        {'name': 'æ¯ 30 åˆ†é’Ÿ', 'expression': '*/30 * * * *', 'description': 'æ¯ 30 åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡'},
        {'name': 'æ¯å°æ—¶', 'expression': '0 * * * *', 'description': 'æ¯å°æ—¶æ•´ç‚¹æ‰§è¡Œ'},
        {'name': 'æ¯ 2 å°æ—¶', 'expression': '0 */2 * * *', 'description': 'æ¯ 2 å°æ—¶æ‰§è¡Œä¸€æ¬¡'},
        {'name': 'æ¯ 6 å°æ—¶', 'expression': '0 */6 * * *', 'description': 'æ¯ 6 å°æ—¶æ‰§è¡Œä¸€æ¬¡'},
        {'name': 'æ¯ 12 å°æ—¶', 'expression': '0 */12 * * *', 'description': 'æ¯ 12 å°æ—¶æ‰§è¡Œä¸€æ¬¡'},
        {'name': 'æ¯å¤©å‡Œæ™¨ 2 ç‚¹', 'expression': '0 2 * * *', 'description': 'æ¯å¤©å‡Œæ™¨ 2:00 æ‰§è¡Œ'},
        {'name': 'æ¯å¤©å‡Œæ™¨ 3 ç‚¹', 'expression': '0 3 * * *', 'description': 'æ¯å¤©å‡Œæ™¨ 3:00 æ‰§è¡Œ'},
        {'name': 'æ¯å¤©æ—©ä¸Š 8 ç‚¹', 'expression': '0 8 * * *', 'description': 'æ¯å¤©æ—©ä¸Š 8:00 æ‰§è¡Œ'},
        {'name': 'æ¯å‘¨ä¸€å‡Œæ™¨ 2 ç‚¹', 'expression': '0 2 * * 1', 'description': 'æ¯å‘¨ä¸€å‡Œæ™¨ 2:00 æ‰§è¡Œ'},
        {'name': 'æ¯æœˆ 1 å·å‡Œæ™¨ 2 ç‚¹', 'expression': '0 2 1 * *', 'description': 'æ¯æœˆ 1 å·å‡Œæ™¨ 2:00 æ‰§è¡Œ'},
        {'name': 'å·¥ä½œæ—¥æ—©ä¸Š 9 ç‚¹', 'expression': '0 9 * * 1-5', 'description': 'å‘¨ä¸€åˆ°å‘¨äº”æ—©ä¸Š 9:00 æ‰§è¡Œ'},
    ]
    return jsonify({'presets': presets})


@app.route('/api/cron/random', methods=['GET'])
def api_cron_random():
    """ç”Ÿæˆéšæœº Cron è¡¨è¾¾å¼"""
    import random
    
    # è·å–å‚æ•°
    pattern = request.args.get('pattern', 'hourly')  # daily, hourly, custom
    
    if pattern == 'daily':
        # æ¯å¤©éšæœºæ—¶é—´
        minute = random.randint(0, 59)
        hour = random.randint(0, 23)
        expression = f"{minute} {hour} * * *"
        description = f"æ¯å¤© {hour:02d}:{minute:02d} æ‰§è¡Œ"
    elif pattern == 'hourly':
        # æ¯å°æ—¶éšæœºåˆ†é’Ÿ
        minute = random.randint(0, 59)
        expression = f"{minute} * * * *"
        description = f"æ¯å°æ—¶çš„ç¬¬ {minute} åˆ†é’Ÿæ‰§è¡Œ"
    elif pattern == 'night':
        # æ·±å¤œéšæœºæ—¶é—´ï¼ˆ23:00-05:00ï¼‰
        minute = random.randint(0, 59)
        hour = random.choice([23, 0, 1, 2, 3, 4, 5])
        expression = f"{minute} {hour} * * *"
        description = f"æ¯å¤©å‡Œæ™¨ {hour:02d}:{minute:02d} æ‰§è¡Œ"
    else:
        # å®Œå…¨éšæœº
        minute = random.randint(0, 59)
        hour = random.randint(0, 23)
        expression = f"{minute} {hour} * * *"
        description = f"æ¯å¤© {hour:02d}:{minute:02d} æ‰§è¡Œ"
    
    return jsonify({
        'expression': expression,
        'description': description,
        'pattern': pattern
    })


@app.route('/api/cron/validate', methods=['POST'])
def api_cron_validate():
    """éªŒè¯ Cron è¡¨è¾¾å¼"""
    data = request.get_json(silent=True) or {}
    expression = data.get('expression', '').strip()
    
    if not expression:
        return jsonify({'valid': False, 'error': 'Cron è¡¨è¾¾å¼ä¸èƒ½ä¸ºç©º'})
    
    parts = expression.split()
    if len(parts) != 5:
        return jsonify({'valid': False, 'error': 'Cron è¡¨è¾¾å¼åº”åŒ…å« 5 ä¸ªå­—æ®µï¼šåˆ† æ—¶ æ—¥ æœˆ æ˜ŸæœŸ'})
    
    try:
        from apscheduler.triggers.cron import CronTrigger
        minute, hour, day, month, day_of_week = parts
        trigger = CronTrigger(
            minute=minute,
            hour=hour,
            day=day,
            month=month,
            day_of_week=day_of_week
        )
        # è·å–ä¸‹æ¬¡æ‰§è¡Œæ—¶é—´
        next_run = trigger.get_next_fire_time(None, datetime.now())
        return jsonify({
            'valid': True,
            'next_run': next_run.isoformat() if next_run else None,
            'description': f"ä¸‹æ¬¡æ‰§è¡Œ: {next_run.strftime('%Y-%m-%d %H:%M:%S')}" if next_run else 'æ— æ³•è®¡ç®—'
        })
    except Exception as e:
        return jsonify({'valid': False, 'error': f'éªŒè¯å¤±è´¥: {str(e)}'})


@atexit.register
def _cleanup():
    if scheduler.is_running:
        scheduler.stop()


def fetch_hitokoto():
    """è·å–ä¸€è¨€"""
    try:
        response = requests.get('https://v1.hitokoto.cn/', timeout=5)
        data = response.json()
        text = data.get('hitokoto', 'ä»Šå¤©ä¹Ÿè¦åŠ æ²¹å“¦ï¼')
        from_who = data.get('from', '')
        return f"{text} â€”â€” {from_who}" if from_who else text
    except Exception as e:
        return 'ä¿æŒçƒ­çˆ±ï¼Œå¥”èµ´å±±æµ·'


if __name__ == '__main__':
    # è·å–ä¸€è¨€
    hitokoto = fetch_hitokoto()
    
    # åªåœ¨é debug æ¨¡å¼æˆ–ä¸»è¿›ç¨‹ä¸­æ˜¾ç¤ºå¯åŠ¨ä¿¡æ¯
    # debug æ¨¡å¼ä¸‹ï¼Œos.environ.get('WERKZEUG_RUN_MAIN') åªåœ¨å­è¿›ç¨‹ä¸­ä¸º 'true'
    if IS_DOCKER or os.environ.get('WERKZEUG_RUN_MAIN') == 'true':
        # å¯åŠ¨ä¿¡æ¯
        print(f'\nâœ… CloudGather v{VERSION} å¯åŠ¨æˆåŠŸ')
        print(f'â° æ—¶åŒº: {os.getenv("TZ", "UTC")}')
        print(f'ğŸŒ è®¿é—®åœ°å€: http://127.0.0.1:3602')
        print(f'ğŸ’¬ ä¸€è¨€: {hitokoto}')
        print('â–¶ï¸  æœåŠ¡è¿è¡Œä¸­... (æŒ‰ CTRL+C åœæ­¢)\n')
    
    # å¯åŠ¨ Flask
    app.run(
        host='0.0.0.0' if IS_DOCKER else '127.0.0.1',
        port=3602,
        debug=not IS_DOCKER
    )
