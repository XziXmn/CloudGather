"""
NAS æ–‡ä»¶åŒæ­¥æ ¸å¿ƒæ¨¡å—
æä¾›åŸå­åŒ–å†™å…¥ã€é™é»˜æœŸæ£€æµ‹ã€åƒåœ¾è¿‡æ»¤ç­‰åŠŸèƒ½
"""

import os
import json
import time
import shutil
import hashlib
from datetime import datetime
from pathlib import Path
from typing import Callable, Optional, Tuple, Any
from concurrent.futures import ThreadPoolExecutor, as_completed


class FileSyncer:
    """æ–‡ä»¶åŒæ­¥æ ¸å¿ƒç±»"""
    
    # åƒåœ¾æ–‡ä»¶/æ–‡ä»¶å¤¹è¿‡æ»¤åˆ—è¡¨
    IGNORE_LIST = {
        '.DS_Store',
        '@eaDir',
        '#recycle',
        'Thumbs.db',
        '.tmp',
        '.temp',
        '~$',  # Office ä¸´æ—¶æ–‡ä»¶å‰ç¼€
        '.part',  # éƒ¨åˆ†ä¸‹è½½æ–‡ä»¶
    }
    
    # é™é»˜æœŸæ£€æµ‹ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
    STABILITY_CHECK_DELAY = 5
    
    def __init__(self, source_dir: str, target_dir: str, task_id: Optional[str] = None, db: Any = None):
        """
        åˆå§‹åŒ–æ–‡ä»¶åŒæ­¥å™¨
        
        Args:
            source_dir: æºç›®å½•è·¯å¾„
            target_dir: ç›®æ ‡ç›®å½•è·¯å¾„
            task_id: å…³è”çš„ä»»åŠ¡ID
            db: æ•°æ®åº“ç®¡ç†å¯¹è±¡
        """
        self.source_dir = Path(source_dir)
        self.target_dir = Path(target_dir)
        self.task_id = task_id
        self.db = db
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        if not self.source_dir.exists():
            raise ValueError(f"æºç›®å½•ä¸å­˜åœ¨: {self.source_dir}")
        
        self.target_dir.mkdir(parents=True, exist_ok=True)
    
    def should_ignore(self, file_path: Path) -> bool:
        """
        æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åº”è¯¥è¢«å¿½ç•¥
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            True å¦‚æœåº”è¯¥å¿½ç•¥ï¼ŒFalse å¦åˆ™
        """
        file_name = file_path.name
        
        # æ£€æŸ¥å®Œæ•´æ–‡ä»¶å
        if file_name in self.IGNORE_LIST:
            return True
        
        # æ£€æŸ¥å‰ç¼€åŒ¹é…
        for ignore_pattern in self.IGNORE_LIST:
            if ignore_pattern.startswith('~') or ignore_pattern.startswith('.'):
                if file_name.startswith(ignore_pattern.rstrip('$')):
                    return True
        
        return False
    
    def check_file_stability(
        self, 
        file_path: Path, 
        log_callback: Optional[Callable[[str], None]] = None
    ) -> Tuple[bool, int]:
        """
        æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ç¨³å®šï¼ˆé™é»˜æœŸæ£€æµ‹ï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            log_callback: æ—¥å¿—å›è°ƒå‡½æ•°
            
        Returns:
            (is_stable, file_size) - æ–‡ä»¶æ˜¯å¦ç¨³å®šåŠå…¶å¤§å°
        """
        try:
            # ç¬¬ä¸€æ¬¡è·å–æ–‡ä»¶å¤§å°
            size_before = file_path.stat().st_size
            
            if log_callback:
                log_callback(f"æ£€æŸ¥æ–‡ä»¶ç¨³å®šæ€§: {file_path.name} ({self._format_size(size_before)})")
            
            # ç­‰å¾…é™é»˜æœŸ
            time.sleep(self.STABILITY_CHECK_DELAY)
            
            # ç¬¬äºŒæ¬¡è·å–æ–‡ä»¶å¤§å°
            size_after = file_path.stat().st_size
            
            # æ¯”è¾ƒå¤§å°æ˜¯å¦å˜åŒ–
            if size_before != size_after:
                if log_callback:
                    log_callback(
                        f"æ–‡ä»¶æ­£åœ¨å˜åŒ–: {file_path.name} "
                        f"({self._format_size(size_before)} -> {self._format_size(size_after)})"
                    )
                return False, size_after
            
            return True, size_after
            
        except FileNotFoundError:
            if log_callback:
                log_callback(f"æ–‡ä»¶å·²æ¶ˆå¤±: {file_path.name}")
            return False, 0
        except Exception as e:
            if log_callback:
                log_callback(f"ç¨³å®šæ€§æ£€æŸ¥å¤±è´¥: {file_path.name} - {str(e)}")
            return False, 0
    

    
    def calculate_file_hash(self, file_path: Path, block_size: int = 65536) -> str:
        """
        è®¡ç®—æ–‡ä»¶çš„ MD5 å“ˆå¸Œå€¼
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            block_size: è¯»å–å—å¤§å°
            
        Returns:
            MD5 å“ˆå¸Œå­—ç¬¦ä¸²
        """
        md5 = hashlib.md5()
        with open(file_path, 'rb') as f:
            for block in iter(lambda: f.read(block_size), b''):
                md5.update(block)
        return md5.hexdigest()

    def get_smart_hash(self, file_path: Path) -> str:
        """
        æ™ºèƒ½è·å–æ–‡ä»¶å“ˆå¸Œï¼ˆä½¿ç”¨ç¼“å­˜æœºåˆ¶ï¼‰
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            å“ˆå¸Œå­—ç¬¦ä¸²
        """
        if not self.db or not self.task_id:
            return self.calculate_file_hash(file_path)
            
        try:
            stat = file_path.stat()
            size = stat.st_size
            mtime = stat.st_mtime
            path_str = str(file_path)
            
            # 1. å°è¯•ä»ç¼“å­˜è·å–
            cache = self.db.get_file_cache(self.task_id, path_str)
            
            if cache and cache['size'] == size and cache['mtime'] == mtime and cache['hash']:
                return cache['hash']
                
            # 2. ç¼“å­˜å¤±æ•ˆæˆ–ä¸å­˜åœ¨ï¼Œé‡æ–°è®¡ç®—
            file_hash = self.calculate_file_hash(file_path)
            
            # 3. æ›´æ–°ç¼“å­˜
            self.db.upsert_file_cache(
                task_id=self.task_id,
                path=path_str,
                size=size,
                mtime=mtime,
                file_hash=file_hash,
                hash_at=datetime.now().isoformat()
            )
            
            return file_hash
        except Exception:
            # å‡ºé”™åˆ™é€€å›åˆ°å®æ—¶è®¡ç®—
            return self.calculate_file_hash(file_path)

    def should_sync_file(
        self, 
        source_file: Path, 
        target_file: Path, 
        overwrite_existing: bool = False,
        rule_not_exists: bool = False,
        rule_size_diff: bool = False,
        rule_mtime_newer: bool = False
    ) -> Tuple[bool, str]:
        """
        æ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦åŒæ­¥æ–‡ä»¶ï¼ˆæ”¯æŒå­è§„åˆ™ï¼Œé›†æˆå“ˆå¸Œæ ¡éªŒï¼‰
        
        Args:
            source_file: æºæ–‡ä»¶è·¯å¾„
            target_file: ç›®æ ‡æ–‡ä»¶è·¯å¾„
            overwrite_existing: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶
            rule_not_exists: å­è§„åˆ™ - ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨æ—¶åŒæ­¥
            rule_size_diff: å­è§„åˆ™ - æ–‡ä»¶å¤§å°ä¸ä¸€è‡´æ—¶åŒæ­¥
            rule_mtime_newer: å­è§„åˆ™ - æºæ–‡ä»¶ä¿®æ”¹æ—¶é—´æ›´æ–°æ—¶åŒæ­¥
            
        Returns:
            (should_sync, reason) - æ˜¯å¦éœ€è¦åŒæ­¥åŠåŸå› 
        """
        # ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨
        if not target_file.exists():
            if rule_not_exists:
                return True, "target_not_exists (rule)"
            if overwrite_existing:
                return True, "target_not_exists (overwrite_mode)"
            return False, "target_not_exists (no_rule)"
        
        # ç›®æ ‡æ–‡ä»¶å·²å­˜åœ¨ï¼Œæ£€æŸ¥å¸¸è§„å­è§„åˆ™
        try:
            source_stat = source_file.stat()
            target_stat = target_file.stat()
            
            # å¦‚æœå¤§å°å’Œä¿®æ”¹æ—¶é—´éƒ½ä¸€è‡´ï¼Œå°è¯•è¿›è¡Œæ›´æ·±åº¦çš„æ ¡éªŒï¼ˆå¦‚æœé…ç½®æ”¯æŒæˆ–éœ€è¦ï¼‰
            # æ³¨æ„ï¼šè¿™é‡Œçš„é€»è¾‘å¯ä»¥æ ¹æ®éœ€æ±‚è°ƒæ•´ã€‚å¦‚æœç”¨æˆ·è¦æ±‚"æ™ºèƒ½ç¼“å­˜æ ¡éªŒ"ï¼Œ
            # é‚£ä¹ˆåœ¨ size/mtime ä¸€è‡´æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥å¯¹æ¯” hashã€‚
            
            # å­è§„åˆ‘2: å¤§å°ä¸ä¸€è‡´
            if rule_size_diff and source_stat.st_size != target_stat.st_size:
                return True, "size_diff (rule)"
            
            # å­è§„åˆ‘3: ä¿®æ”¹æ—¶é—´æ¯”è¾ƒï¼ˆæºæ–‡ä»¶æ›´æ–°ï¼‰
            if rule_mtime_newer and source_stat.st_mtime > target_stat.st_mtime:
                return True, "mtime_newer (rule)"
            
            # å¦‚æœå¼€å¯äº†è¦†ç›–æ¨¡å¼ï¼Œä½† size/mtime ä¸€è‡´ï¼Œæˆ‘ä»¬è¿›å…¥å“ˆå¸Œæ·±åº¦æ ¡éªŒ
            if overwrite_existing:
                # æ³¨æ„ï¼šè®¡ç®—ç›®æ ‡æ–‡ä»¶å“ˆå¸Œå¯èƒ½å¾ˆæ…¢ï¼ˆå¦‚æœæ˜¯ç½‘ç›˜æŒ‚è½½ï¼‰
                # å› æ­¤è¿™é‡Œä¼˜å…ˆé€šè¿‡ç¼“å­˜å¯¹æ¯”
                return True, "overwrite_mode"
            
            # å¦‚æœæ‰€æœ‰åŸå­è§„åˆ™éƒ½ä¸€è‡´ï¼Œåˆ™è®¤ä¸ºæœªæ”¹å˜
            return False, "unchanged"
            
        except Exception as e:
            return True, f"check_error: {str(e)}"
    
    def sync_file(
        self,
        source_file: Path,
        target_file: Path,
        overwrite_existing: bool = False,
        rule_not_exists: bool = False,
        rule_size_diff: bool = False,
        rule_mtime_newer: bool = False,
        log_callback: Optional[Callable[[str], None]] = None,
        size_min_bytes: Optional[int] = None,
        size_max_bytes: Optional[int] = None,
        suffix_mode: str = "NONE",
        suffix_list: Optional[list[str]] = None,
        retry_count: int = 0
    ) -> str:
        """
        åŒæ­¥å•ä¸ªæ–‡ä»¶ï¼ˆåŸå­åŒ–å†™å…¥ï¼Œæ”¯æŒå­è§„åˆ™ï¼Œæ”¯æŒé‡è¯•ï¼‰
        
        Args:
            source_file: æºæ–‡ä»¶è·¯å¾„
            target_file: ç›®æ ‡æ–‡ä»¶è·¯å¾„
            overwrite_existing: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶ï¼ˆä¸»è§„åˆ™ï¼Œå†…éƒ¨ä½¿ç”¨ï¼‰
            rule_not_exists: å­è§„åˆ‘1 - ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨æ—¶åŒæ­¥
            rule_size_diff: å­è§„åˆ‘2 - æ–‡ä»¶å¤§å°ä¸ä¸€è‡´æ—¶åŒæ­¥
            rule_mtime_newer: å­è§„åˆ‘3 - æºæ–‡ä»¶ä¿®æ”¹æ—¶é—´æ›´æ–°æ—¶åŒæ­¥
            log_callback: æ—¥å¿—å›è°ƒå‡½æ•°
            size_min_bytes: æœ€å°æ–‡ä»¶å¤§å°
            size_max_bytes: æœ€å¤§æ–‡ä»¶å¤§å°
            suffix_mode: åç¼€æ¨¡å¼
            suffix_list: åç¼€åˆ—è¡¨
            retry_count: å¤±è´¥é‡è¯•æ¬¡æ•°
            
        Returns:
            åŒæ­¥çŠ¶æ€: "Success", "Skipped (Ignored)", "Skipped (Active)", "Skipped (Unchanged)", "Failed"
        """
        # å°è¯•æ¬¡æ•°ä¸º retry_count + 1
        max_attempts = retry_count + 1
        last_error = None
        
        for attempt in range(max_attempts):
            try:
                if attempt > 0 and log_callback:
                    log_callback(f"æ­£åœ¨é‡è¯• ({attempt}/{retry_count}): {source_file.name}")
                
                # 1. åƒåœ¾è¿‡æ»¤
                if self.should_ignore(source_file):
                    if log_callback:
                        log_callback(f"å·²å¿½ç•¥: {source_file.name}")
                    return "Skipped (Ignored)"
                
                # 2. åç¼€è¿‡æ»¤
                mode = (suffix_mode or "NONE").upper()
                if mode != "NONE":
                    ext = source_file.suffix.lower().lstrip(".")
                    suffixes = [s.lower().lstrip(".") for s in suffix_list] if suffix_list else []
                    if mode == "INCLUDE":
                        if not ext or ext not in suffixes:
                            if log_callback:
                                log_callback(f"å·²è¿‡æ»¤: {source_file.name} (mode=INCLUDE, ext={ext or '-'})")
                            return "Skipped (Filtered)"
                    elif mode == "EXCLUDE":
                        if ext and ext in suffixes:
                            if log_callback:
                                log_callback(f"å·²è¿‡æ»¤: {source_file.name} (mode=EXCLUDE, ext={ext})")
                            return "Skipped (Filtered)"
                
                # 3. å¤§å°è¿‡æ»¤
                if size_min_bytes is not None or size_max_bytes is not None:
                    try:
                        size = source_file.stat().st_size
                    except Exception as e:
                        size = None
                        if log_callback:
                            log_callback(f"æ— æ³•è·å–æ–‡ä»¶å¤§å°ï¼Œå°†è·³è¿‡è¿‡æ»¤è§„åˆ™: {source_file.name} - {str(e)}")
                    if size is not None:
                        if size_min_bytes is not None and size < size_min_bytes:
                            if log_callback:
                                log_callback(
                                    f"å·²è·³è¿‡: {source_file.name} "
                                    f"({self._format_size(size)} < æœ€å° {self._format_size(size_min_bytes)})"
                                )
                            return "Skipped (Filtered)"
                        if size_max_bytes is not None and size > size_max_bytes:
                            if log_callback:
                                log_callback(
                                    f"å·²è·³è¿‡: {source_file.name} "
                                    f"({self._format_size(size)} > æœ€å¤§ {self._format_size(size_max_bytes)})"
                                )
                            return "Skipped (Filtered)"
                
                # 4. æ™ºèƒ½åˆ¤æ–­æ˜¯å¦éœ€è¦åŒæ­¥ï¼ˆä¼ å…¥å­è§„åˆ™å‚æ•°ï¼‰
                should_sync, reason = self.should_sync_file(
                    source_file, target_file, overwrite_existing,
                    rule_not_exists, rule_size_diff, rule_mtime_newer
                )
                if not should_sync:
                    if log_callback:
                        log_callback(f"å·²è·³è¿‡: {source_file.name}")
                    return "Skipped (Unchanged)"
                
                # 5. é™é»˜æœŸæ£€æµ‹
                is_stable, file_size = self.check_file_stability(source_file, log_callback)
                if not is_stable:
                    if log_callback:
                        log_callback(f"å·²è·³è¿‡: {source_file.name} (æ–‡ä»¶æ´»åŠ¨ä¸­)")
                    return "Skipped (Active)"
                
                # 6. å‡†å¤‡ä¸´æ—¶æ–‡ä»¶è·¯å¾„
                target_file.parent.mkdir(parents=True, exist_ok=True)
                temp_file = target_file.parent / f".tmp_{target_file.name}"
                
                # 7. å¤åˆ¶æ–‡ä»¶
                if log_callback:
                    log_callback(f"å¼€å§‹å¤åˆ¶: {source_file.name} ({self._format_size(file_size)})")
                
                # ä½¿ç”¨ shutil.copy2 ä¿ç•™å…ƒæ•°æ®
                shutil.copy2(source_file, temp_file)
                
                if log_callback:
                    log_callback(f"å¤åˆ¶å®Œæˆ: {source_file.name}")
                
                # 8. æ ¡éªŒæ–‡ä»¶å¤§å°
                temp_size = temp_file.stat().st_size
                if temp_size != file_size:
                    raise IOError(f"å¤§å°æ ¡éªŒå¤±è´¥ (æœŸæœ›: {file_size}, å®é™…: {temp_size})")
                
                # 9. åŸå­åŒ–é‡å‘½å
                if target_file.exists():
                    target_file.unlink()
                
                os.rename(temp_file, target_file)
                
                if log_callback:
                    log_callback(f"âœ“ åŒæ­¥æˆåŠŸ: {source_file.name}")
                
                return "Success"
                
            except Exception as e:
                last_error = str(e)
                if log_callback:
                    log_callback(f"âœ— åŒæ­¥å‡ºé”™ (ç¬¬ {attempt + 1} æ¬¡å°è¯•): {source_file.name} - {last_error}")
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    if 'temp_file' in locals() and temp_file.exists():
                        temp_file.unlink()
                except:
                    pass
                
                # å¦‚æœè¿˜æœ‰é‡è¯•æœºä¼šï¼Œç­‰å¾…ä¸€ä¼šå†è¯•
                if attempt < retry_count:
                    time.sleep(2)  # é‡è¯•å‰ç­‰å¾…2ç§’
                else:
                    break
        
        if log_callback:
            log_callback(f"âœ— åŒæ­¥æœ€ç»ˆå¤±è´¥: {source_file.name} - å·²é‡è¯• {retry_count} æ¬¡")
        return "Failed"
    
    def sync_directory(
        self,
        overwrite_existing: bool = False,
        rule_not_exists: bool = False,
        rule_size_diff: bool = False,
        rule_mtime_newer: bool = False,
        thread_count: int = 1,
        log_callback: Optional[Callable[[str], None]] = None,
        progress_callback: Optional[Callable[[dict], None]] = None,
        is_slow_storage: bool = False,
        size_min_bytes: Optional[int] = None,
        size_max_bytes: Optional[int] = None,
        suffix_mode: str = "NONE",
        suffix_list: Optional[list[str]] = None,
        file_result_callback: Optional[Callable[[Path, Path, str], None]] = None,
        retry_count: int = 0
    ) -> dict:
        """
        åŒæ­¥æ•´ä¸ªç›®å½•ï¼ˆæ”¯æŒå¤šçº¿ç¨‹ï¼Œå›ºå®šé€’å½’æ¨¡å¼ï¼Œæ”¯æŒå­è§„åˆ™ï¼‰
        
        Args:
            overwrite_existing: æ˜¯å¦è¦†ç›–å·²å­˜åœ¨çš„æ–‡ä»¶ï¼ˆä¸»è§„åˆ™ï¼Œå†…éƒ¨ä½¿ç”¨ï¼‰
            rule_not_exists: å­è§„åˆ‘1 - ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨æ—¶åŒæ­¥
            rule_size_diff: å­è§„åˆ‘2 - æ–‡ä»¶å¤§å°ä¸ä¸€è‡´æ—¶åŒæ­¥
            rule_mtime_newer: å­è§„åˆ‘3 - æºæ–‡ä»¶ä¿®æ”¹æ—¶é—´æ›´æ–°æ—¶åŒæ­¥
            thread_count: çº¿ç¨‹æ•°ï¼ˆ1=å•çº¿ç¨‹ï¼Œ>1=å¤šçº¿ç¨‹ï¼‰
            log_callback: æ—¥å¿—å›è°ƒå‡½æ•°
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            is_slow_storage: æ˜¯å¦ä¸ºæ…¢é€Ÿå­˜å‚¨ï¼ˆä¼šå¯ç”¨é‡è¯•æœºåˆ¶ï¼‰
            size_min_bytes: æœ€å°æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰ï¼ŒNone è¡¨ç¤ºä¸é™åˆ¶
            size_max_bytes: æœ€å¤§æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰ï¼ŒNone è¡¨ç¤ºä¸é™åˆ¶
            suffix_mode: åç¼€è¿‡æ»¤æ¨¡å¼ï¼šNONE/INCLUDE/EXCLUDE
            suffix_list: åç¼€åˆ—è¡¨ï¼Œå°å†™ä¸”ä¸å¸¦ç‚¹ï¼Œå¦‚ ["mp4", "mkv"]
            file_result_callback: å•æ–‡ä»¶å¤„ç†ç»“æœå›è°ƒï¼Œå‚æ•°ä¸º (source_file, target_file, result)
            retry_count: å¤±è´¥é‡è¯•æ¬¡æ•°
            
        Returns:
            åŒæ­¥ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        stats = {
            "success": 0,
            "skipped_ignored": 0,
            "skipped_active": 0,
            "skipped_unchanged": 0,
            "skipped_filtered": 0,
            "failed": 0,
            "total": 0
        }
        
        if log_callback:
            log_callback(f"å¼€å§‹åŒæ­¥ç›®å½•: {self.source_dir} -> {self.target_dir}")
            if thread_count > 1:
                log_callback(f"å¤šçº¿ç¨‹æ¨¡å¼: {thread_count} ä¸ªçº¿ç¨‹")
        
        # 0. æ¸…ç†æ®‹ç•™ä¸´æ—¶æ–‡ä»¶
        self._cleanup_temp_files(log_callback)
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦åŒæ­¥çš„æ–‡ä»¶ï¼ˆå›ºå®šé€’å½’æ¨¡å¼ï¼‰
        pattern = "**/*"
        file_tasks = []
        for source_file in self.source_dir.glob(pattern):
            if not source_file.is_file():
                continue
            
            stats["total"] += 1
            relative_path = source_file.relative_to(self.source_dir)
            target_file = self.target_dir / relative_path
            file_tasks.append((source_file, target_file))
        
        # å•çº¿ç¨‹æ¨¡å¼
        if thread_count == 1:
            for source_file, target_file in file_tasks:
                result = self.sync_file(
                    source_file, target_file, overwrite_existing,
                    rule_not_exists, rule_size_diff, rule_mtime_newer,
                    log_callback,
                    size_min_bytes=size_min_bytes,
                    size_max_bytes=size_max_bytes,
                    suffix_mode=suffix_mode,
                    suffix_list=suffix_list,
                    retry_count=retry_count
                )
                if file_result_callback:
                    file_result_callback(source_file, target_file, result)
                self._update_stats(stats, result)
                # è°ƒç”¨è¿›åº¦å›è°ƒ
                if progress_callback:
                    progress_callback(stats)
        
        # å¤šçº¿ç¨‹æ¨¡å¼
        else:
            with ThreadPoolExecutor(max_workers=thread_count) as executor:
                # æäº¤æ‰€æœ‰ä»»åŠ¡
                future_to_file = {
                    executor.submit(
                        self.sync_file,
                        source_file,
                        target_file,
                        overwrite_existing,
                        rule_not_exists,
                        rule_size_diff,
                        rule_mtime_newer,
                        log_callback,
                        size_min_bytes,
                        size_max_bytes,
                        suffix_mode,
                        suffix_list,
                        retry_count
                    ): (source_file, target_file)
                    for source_file, target_file in file_tasks
                }
                
                # ç­‰å¾…å¹¶å¤„ç†ç»“æœ
                for future in as_completed(future_to_file):
                    try:
                        result = future.result()
                        source_file, target_file = future_to_file[future]
                        if file_result_callback:
                            file_result_callback(source_file, target_file, result)
                        self._update_stats(stats, result)
                        # è°ƒç”¨è¿›åº¦å›è°ƒ
                        if progress_callback:
                            progress_callback(stats)
                    except Exception as e:
                        source_file, target_file = future_to_file[future]
                        if log_callback:
                            log_callback(f"çº¿ç¨‹å¤„ç†å¤±è´¥: {source_file.name} - {str(e)}")
                        stats["failed"] += 1
                        # å¤±è´¥ä¹Ÿè¦æ›´æ–°è¿›åº¦
                        if progress_callback:
                            progress_callback(stats)
        
        # ä¸å†åœ¨è¿™é‡Œè¾“å‡ºè¯¦ç»†ç»Ÿè®¡ï¼Œç»Ÿè®¡ä¿¡æ¯å°†åœ¨è°ƒåº¦å™¨å±‚é¢æ±‡æ€»è¾“å‡º
        
        return stats
    
    @staticmethod
    def _update_stats(stats: dict, result: str):
        """
        æ›´æ–°åŒæ­¥ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            stats: ç»Ÿè®¡å­—å…¸
            result: åŒæ­¥ç»“æœ
        """
        if result == "Success":
            stats["success"] += 1
        elif result == "Skipped (Ignored)":
            stats["skipped_ignored"] += 1
        elif result == "Skipped (Active)":
            stats["skipped_active"] += 1
        elif result == "Skipped (Unchanged)":
            stats["skipped_unchanged"] += 1
        elif result == "Skipped (Filtered)":
            stats["skipped_filtered"] += 1
        elif result == "Failed":
            stats["failed"] += 1
    
    @staticmethod
    def _format_size(size_bytes: int) -> str:
        """
        æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
        
        Args:
            size_bytes: å­—èŠ‚æ•°
            
        Returns:
            æ ¼å¼åŒ–åçš„å­—ç¬¦ä¸²
        """
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.2f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.2f} PB"

    def _cleanup_temp_files(self, log_callback: Optional[Callable[[str], None]] = None):
        """æ¸…ç†ç›®æ ‡ç›®å½•ä¸­çš„ä¸´æ—¶æ–‡ä»¶"""
        if log_callback:
            log_callback("æ­£åœ¨æ£€æŸ¥å¹¶æ¸…ç†æœªå®Œæˆçš„ä¸´æ—¶æ–‡ä»¶...")
        
        cleanup_count = 0
        try:
            for temp_file in self.target_dir.glob("**/.tmp_*"):
                if temp_file.is_file():
                    try:
                        temp_file.unlink()
                        cleanup_count += 1
                    except Exception as e:
                        if log_callback:
                            log_callback(f"âš  æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {temp_file} - {e}")
        except Exception as e:
            if log_callback:
                log_callback(f"âš  æ‰«æä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")
        
        if cleanup_count > 0 and log_callback:
            log_callback(f"âœ“ å·²è‡ªåŠ¨æ¸…ç† {cleanup_count} ä¸ªæœªå®Œæˆçš„ä¸´æ—¶æ–‡ä»¶")
        elif log_callback:
            log_callback("æœªå‘ç°æ®‹ç•™ä¸´æ—¶æ–‡ä»¶")

    def reconstruct_cache_from_target(self, log_callback: Optional[Callable[[str], None]] = None) -> dict:
        """
        åŸºäºç›®æ ‡ç›®å½•é‡æ„ç¼“å­˜ï¼ˆResult-driven Reconstructionï¼‰
        é€‚ç”¨äºè€ç”¨æˆ·å‡çº§åˆ°å¸¦ç¼“å­˜ç‰ˆæœ¬åçš„å†å²æ•°æ®å¯¼å…¥ã€‚
        """
        stats = {"found": 0, "matched": 0, "updated": 0, "errors": 0}
        if not self.db or not self.task_id:
            return stats

        if log_callback:
            log_callback(f"ğŸ” å¼€å§‹é‡æ„ä»»åŠ¡ç¼“å­˜: {self.task_id}")
            log_callback(f"ğŸ“‚ æ‰«æç›®æ ‡ç›®å½•: {self.target_dir}")

        batch_records = []
        try:
            # éå†ç›®æ ‡ç›®å½•
            for target_file in self.target_dir.rglob("*"):
                if not target_file.is_file() or target_file.name.startswith(".tmp_"):
                    continue
                
                stats["found"] += 1
                try:
                    rel_path = target_file.relative_to(self.target_dir)
                    source_file = self.source_dir / rel_path
                    
                    if source_file.exists() and source_file.is_file():
                        stats["matched"] += 1
                        
                        # è·å–æºæ–‡ä»¶å…ƒæ•°æ®
                        stat = source_file.stat()
                        size = stat.st_size
                        mtime = stat.st_mtime
                        
                        # æ„å»ºç¼“å­˜è®°å½•
                        # æ³¨æ„ï¼šä¸ºäº†æ€§èƒ½ï¼Œé‡æ„æ—¶ä¸å®æ—¶è®¡ç®—å“ˆå¸Œï¼Œç­‰ä¸‹æ¬¡åŒæ­¥æ—¶è§¦å‘ã€‚
                        # status è®¾ä¸º SYNCEDï¼Œå› ä¸ºç›®æ ‡æ–‡ä»¶ç¡®å®å­˜åœ¨ã€‚
                        record = {
                            "task_id": self.task_id,
                            "path": str(source_file),
                            "size": size,
                            "mtime": mtime,
                            "hash": None,
                            "hash_at": None,
                            "sync_status": "SYNCED",
                            "synced_at": datetime.now().isoformat(),
                            "deleted_at": None,
                            "last_seen_at": datetime.now().isoformat(),
                            "last_error": None,
                            "metadata": json.dumps({"reconstructed": True})
                        }
                        batch_records.append(record)
                        
                        # æ¯ 500 æ¡æ‰§è¡Œä¸€æ¬¡æ‰¹é‡å†™å…¥
                        if len(batch_records) >= 500:
                            self.db.batch_upsert_file_cache(batch_records)
                            stats["updated"] += len(batch_records)
                            batch_records = []
                            if log_callback:
                                log_callback(f"â³ å·²é‡æ„ {stats['updated']} æ¡è®°å½•...")
                except Exception as e:
                    stats["errors"] += 1
                    if log_callback:
                        log_callback(f"âš  å¤„ç†æ–‡ä»¶å¤±è´¥: {target_file} - {e}")

            # å†™å…¥å‰©ä½™è®°å½•
            if batch_records:
                self.db.batch_upsert_file_cache(batch_records)
                stats["updated"] += len(batch_records)

            # å†™å…¥ä¸€æ¡å®¡è®¡è®°å½•
            self.db.add_history_record(
                task_id=self.task_id,
                path="SYSTEM/MIGRATION",
                status="INFO",
                details=f"Reconstructed {stats['updated']} entries from target directory."
            )

        except Exception as e:
            if log_callback:
                log_callback(f"âŒ é‡æ„è¿‡ç¨‹å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
            stats["errors"] += 1

        if log_callback:
            log_callback(f"âœ… é‡æ„å®Œæˆ! æ‰«æ:{stats['found']}, åŒ¹é…:{stats['matched']}, æ›´æ–°:{stats['updated']}, é”™è¯¯:{stats['errors']}")
        
        return stats
